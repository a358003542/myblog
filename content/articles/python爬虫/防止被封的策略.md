Title: python爬虫之-防止被封的策略
Date: 2018-06-14
[TOC]



防止被封的首要策略就是尽量的为别人的服务器多考虑下，让自己写的爬虫少请求，每次请求都是有效的核心请求获取最核心的数据，不管是刷页面还是刷ajax，多次请求之间应该设置一个停顿时间。



```
time.sleep(3)
```





在上面的首要原则的基础上，下面介绍的很多实战技巧，其实都符合一个大的原则：尽可能地让你的爬虫和人浏览网页没有区别。



## http请求头调整

user-agent 设置，而且时不时的切换下。

虽然目前还没有遇到，不过我推测可能 Referrer 这个header在某些场景下是有些文章的。

还有 Accept-Language 也可能有用。



## Cookies

有些情况下cookies的正常获取需要javascript的支持。cookies总的原则是第一次请求获取到cookies，然后后面的很多次请求都使用这个cookies即可。

不过反爬虫cookies一般都会有个时间限制，一个简单的做法就是这边也设置个时间，定时获取最新的cookies或者，一定请求量之后再获取一个新的cookies。

具体使用 scrapy-splash 了解下。



## 表单陷阱

有的表单里面有：

```
<input type="hidden" ...
```

我们要记住人如果在页面上点击，这个没有显示的字段的值也会一并送过去，而他们服务器那边会根据这个值可能是个加密的某个值来判断这个请求是人点的还是爬虫行为。

最好的策略是先把整个表单内容爬过来，收集好之后再发送表单请求。

和上面的情况相反，还有一种情况，页面表单发送可能有特别的处理，某些表单字段，不管用户看得见看不见，你都不能发送过去，只要发送过去就会被毙掉。

继续上面的表单陷阱，某些css也会动态将某个input 属于hidden属性，这个需要好好分析下。



## 403 forbidden

这极有可能是你的爬虫被封了。







## 参考资料

- web scraping with python   writing by Ryan Mitchell