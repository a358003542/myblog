Title: python语言学习之-多进程多线程
Date: 2017-03-03
[TOC]



进程的定义是:
一个正在执行的程序实例。每个进程都有一个唯一的进程ID，也就是所谓的
**PID** 。使用`ps`
命令的第一个列就是每个进程的PID属性。在python中你可以使用`os.getpid()`来查看当前进程的PID。

以前只有一个CPU的机器上，多任务操作系统实际上一次也只能运行一个进程，操作系统是通过不断切换各个进程给你一种多任务似乎同时在运行多个程序的感觉的。多CPU机器上是真的可以同时运行多个进程。

## 进程fork

进程fork简单来说就类似于git某个项目的fork，进行了一些基本代码信息和其他配置以及其他相关信息的复制或注册。这就相当于在当前代码环境下，你有两个分别单独运行的程序实例了。

下面是一个非常简单的小例子，你可以把os.fork()语句移到print('before
fork')之前来看看变化。

    import os, time
    
    print('before fork ')
    os.fork()
    
    print('say hello from', os.getpid())
    
    time.sleep(1)
    
    print('after fork')

对于这个程序简单的理解就是，本py文件编译成字节码进入内存经过某些成为一个程序实例了（其中还包含其他一些信息），然后程序具体运行的时候会通过os.fork来调用系统的fork函数，然后复制本程序实例（以本程序实例目前已经所处的状态），因为print('before
fork')已经执行了，所以子进程就不会执行这一行代码了，而是继续os.fork()下面的代码继续执行。此时就相当于有两个程序在运行了，至于后面的打印顺序那是说不准的。

关于操作系统具体如何fork的我们可以暂时不考虑，这两个程序实例里面的变量和运行环境基本上是一模一样的，除了运行的状态有所不同之外。fork可以做出一种程序多任务处理方案吧，不过os模块的fork方法目前只支持unix环境。

## 子进程和父进程分开

请看下面的代码:

    import os, time
    
    print('before fork ')
    pid = os.fork()
    if pid:
        print(pid)
        print('say hello from parent', os.getpid())
    else:
        print(pid)
        print('say hello from child', os.getpid())
    
    time.sleep(1)
    
    print('after fork')

其运行结果大致如下:

    before fork 
    13762
    say hello from parent 13761
    0
    say hello from child 13762
    after fork
    after fork

我们看到在父进程那一边，pid是本父进程的子进程PID，而在子进程那一边，os.fork()返回的是0。可以利用这点将父进程的操作和子进程的操作分开。具体上面的代码if
pid 那一块是父进程的，else那一块是子进程的。

## 线程入门

线程的内部实施细节其实比进程要更加复杂，一般通俗的说法就是线程是轻量级进程，这里不深入讨论具体线程的细节。

python操作线程的主要模块是**threading**模块，简单的使用就是新建一个线程对象(Thread)，然后调用**start**方法来启动它，具体线程要做些什么由本线程对象的**run**确定，你可以重定义它，如果是默认的就是调用本线程Thread类新建是输入的**target**参数，这个target参数具体指向某个函数。下面是一个简单的例子:

    import random, threading
    
    result = []
    
    def randchar_number(i):
        number_list = list(range(48,58))
        coden = random.choice(number_list)
        result.append(chr(coden))
        print('thread:', i)
    
    for i in range(8):
        t = threading.Thread(target = randchar_number, args=(i,))
        t.start()
    
    print(''.join(result))
    
    thread: 0
    thread: 1
    thread: 2
    thread: 3
    thread: 4
    thread: 5
    thread: 6
    thread: 7
    22972371

*注意:* 控制参数后面那个逗号必须加上。

我不太喜欢这种风格，因为线程对接的那个函数实际上并不能return
什么值，而且其保存的值也依赖于前面的定义，并不能称之为真正意义上的函数（一个定义很好的函数必须复用特性很强）。所以线程还是如下类的风格编写。下面代码参考了
[这个网页](http://www.ibm.com/developerworks/aix/library/au-threadingpython/index.html)。

    import random, threading
    
    threads = []
    
    class MyThread(threading.Thread):
        def __init__(self):
            threading.Thread.__init__(self)
            self.result = ''
        def run(self):
            number_list = list(range(48,58))
            coden = random.choice(number_list)
            self.result = chr(coden)
        def getvalue(self):
            return self.result


    for i in range(8):
        t = MyThread()
        t.start()
        t.join()
        threads.append(t)
    
    result = ''
    for t in threads:
        result += t.getvalue()
    print(result)
    
    05649040
    >>> 

上面调用线程对象的 **join**
方法是确保该线程执行完了，其也可能返回异常。上面的做法不太标准，更标准的做法是单独写一行t.join代码:

    for t in threads:
        t.join()

来确保各个线程都执行完了，如之前的形式并不能达到多任务并行处理的效果。

上面的例子对线程的执行顺序没有特殊要求，如果有的话推荐使用python的queue模块，这里就略过了。

## 后台线程

下面的函数实现了一个后台警报线程，不会阻塞主程序。

    def beep(a,b):
        '''make a sound , 
        ref: http://stackoverflow.com/questions/16573051/
            python-sound-alarm-when-code-finishes
        you need install  ``apt-get install sox``
    
        :param a: frenquency
        :param b: duration
    
        create a background thread,so this function does not block
        '''
        def _beep(a,b):
            import os
            os.system('play --no-show-progress --null --channels 1 \
                synth %s sine %f' % (b,a))
        from threading import Thread
        thread = Thread(target=_beep,args=(a,b))
        thread.daemon = True
        thread.start()

如上所示，原beep函数调用系统的play命令制造一个声音，其中b是声音持续的时间，所以其是阻塞的。我们将其作为一个线程调用之后，然后其就没有阻塞主程序了。这里的
`daemon` 的意思是让这个线程成为一个后台线程，请参看
[这个网页](http://stackoverflow.com/questions/190010/daemon-threads-explanation)
，其说道后台线程可以不用管了，后面会随着主程序自动关闭。

## 多线程: 一个定时器

这个例子主要参考了[这个网页](https://mail.python.org/pipermail/tutor/2004-November/033333.html)。

    #!/usr/bin/env python3
    # -*- coding: utf-8 -*-
    import time
    import threading
    
    class Timer(threading.Thread):
        def __init__(self,interval, action=lambda:print('\a')):
            threading.Thread.__init__(self)
            self.interval = interval
            self.action = action
    
        def run(self):
            time.sleep(self.interval)
            self.action()
    
        def set_interval(self,interval):
            self.interval = interval
    
    #timer = Timer(5)
    #timer.start()
    
    class CountDownTimer(Timer):
        def run(self):
            counter = self.interval
            for sec in range(self.interval):
                print(counter)
                time.sleep(1.0)
                counter -= 1
            ##
            self.action()
    
    #timer = CountDownTimer(5)
    #timer.start()
    
    def hello():
        print('hello\a')
    
    timer = CountDownTimer(5, action = hello)
    timer.start()

具体还是很简单的，这里之所以使用线程就是为了timer.sleep函数不冻结主程序。

## 多线程下载大文件

本小节参考了
[这个网页](http://stackoverflow.com/questions/13973188/requests-with-multiple-connections)
和
[这个网页](http://stackoverflow.com/questions/16694907/how-to-download-large-file-in-python-with-requests-py)
。

下面的 `get_content_tofile`
函数在目标内容大小大于1M的时候将启动多线程下载方法。其中
`guess_url_filename`
函数是根据url来猜测可能的目标下载文件名字，还只是一个尝试版本。

注意下面使用requests.get函数的时候加上了
`stream=True`参数，这样连接目标url的时候只是获得头文件信息而不会进一步下载content内容。这方便我们早期根据headers里面的信息做出一些判断。

接下来根据HTTP头文件的 `content-length`
来判断要下载内容的大小，如果没有这个属性，那么目标url是没有content内容的，本函数将不会对这一情况做出反应，这通常是单网页url，使用requests的get方法获取网页文本内容即可。

然后如果目标长度小于1M，那么就直接打开文件，使用requests模块里response对象的`iter_content`方法来不断迭代完content内容。

如果目标长度大于1M，则采用一种多线程下载方法。首先是`get_content_partly`这个函数，接受url和index，这个index是一个简单的索引，具体多少bytes后面还需要计算。关于多线程操作和具体多少bytes的计算细节这里略过讨论了。唯一值得一提的就是HTTP协议的Range属性，begin-end，对应具体的范围0-1024，还包括1024位，所以实际上有1025个bytes，为了获得和我们python中一致的体验，我们让其end为begin+1024-1。这样就有1024个bytes位，然后定位是(0,
1024)，即和python中的一样，不包括1024位。

然后还有一个小信息是，HTTP协议返回的头文件中的**content-range**属性，如果你请求Range越界了，那么将不会有这个属性。那么begin没有越界，end越界的请求如何呢？HTTP协议处理得很好，这种跨界情况都只返回最后那点content内容。

最后写文件那里降低内存消耗，使用了下面的语句来强制文件流写入文件中，好释放内存，否则你的下载程序内存使用率是剧增的。

    f.flush()
    os.fsync(f.fileno())
    
    import re
    def guess_url_filename(url):
        '''根据url来猜测可能的目标文件名，'''
        response = requests.get(url, stream=True)##还有一个content-type信息可以利用
        s = urlsplit(url)
        guess_element = s.path.split('/')[-1]
        guess_pattern = re.compile(r'''
        (.png|.flv)
        $           # end of string
        ''', re.VERBOSE | re.IGNORECASE)
    
        if re.search(guess_pattern,guess_element):
            filename = guess_element
        else:
            filename = guess_element + '.html'
        return filename
    
    import threading
    import os
    class DownloadThread(threading.Thread):
        def __init__(self, url,begin,chunk_size = 1024*300):
            threading.Thread.__init__(self)
            self.url = url
            self.begin = begin
            self.chunk_size = chunk_size
            self.result = b''
        def run(self):
            headers = {'Range':'bytes={begin}-{end}'.format(begin = str(self.begin),
                end = str(self.begin + self.chunk_size-1))}
    
            response = requests.get(url, stream=True, headers = headers)
    
            if response.headers.get('content-range') is None:
                self.result = 0##表示已经越界了
            else:
                self.result = response.content
                print('start download...', self.begin/1024, 'KB')
    
        def getvalue(self):
            return self.result
    
    def get_content_partly(url, index):
        threads = []
        content = b''
        chunk_size = 1024*300# 这个不能设置太大也不能设置太小
        block_size = 10*chunk_size# 具体线程数
    
        for i in range(10):
            t = DownloadThread(url, index * block_size + i*chunk_size )
            t.start()
            threads.append(t)
    
        for i,t in enumerate(threads):
            t.join()
    
        for t in threads:
            if  t.getvalue():
                content += t.getvalue()
    
        return content
    
    import os
    def get_content_tofile(url,filename = ''):
        '''简单的根据url获取content，并将其存入内容存入某个文件中。
        如果某个内容size 小于1M 1000000 byte ，则采用多线程下载法'''
    
        if not filename:
            filename = guess_url_filename(url)
    
        # NOTE the stream=True parameter
        response = requests.get(url, stream=True)
        if not response.headers.get('content-length'):
            print('this url does not have a content .')
            return 0
        elif response.headers.get('content-length') < '1000000':
            with open(filename, 'wb') as f:
                for chunk in response.iter_content(chunk_size=1024):
                    if chunk: # filter out keep-alive new chunks
                        f.write(chunk)
                        f.flush()
                        os.fsync(f.fileno())
        else:
            with open(filename, 'wb') as f:
                for i in range(1000000):##very huge
                    content = get_content_partly(url, i)
                    if content:
                        f.write(content)
                        f.flush()
                        os.fsync(f.fileno())
                    else:
                        print('end...')
                        break



## 线程锁

python有两种类型线程锁 `Lock` 和 `RLock` ，其都是通过 `acquire` 来获取锁和 `release` 来释放锁。当一个线程试着访问某个unlocked的锁，`acquire` 将立即返回；如果访问的是locked的锁，那么该线程将阻塞，直到一个 `release` 释放了该锁。

RLock和Lock的区别是RLock可以被相同的线程acquire多次，RLock人们也称之为递归锁，如果你的某个（递归）函数在某个线程中多次访问资源，而这时被允许的，那么你应该使用RLock。

RLock常和with语句一起使用：

```
lock = threading.RLock()
with lock:
    do something...
```

