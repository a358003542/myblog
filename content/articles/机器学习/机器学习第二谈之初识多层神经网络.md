Title: 机器学习第二谈之初识多层神经网络
Slug: machine-learning-talk-two
Date: 2018-11-05
Modified: 2018-11-05
Tags: machine-learning

[TOC]

## 前言

本文先用tensorflow来实现单层神经网络处理mnist问题，然后用keras来写一个两层神经网络来解决mnist问题。最后试着用keras编写一个简单的深度学习模型，也就是多层神经网络来解决mnist问题。



本文代码主要参考了keras的examples代码库，同时本文也考虑了一些输入数据的预处理统一化过程。



## 数据预处理

首先我们利用keras来下载mnist相关数据并进行必要的预处理操作。

```
from keras.datasets import mnist
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()
```

```
train_images.shape
(60000, 28, 28)
train_labels.shape
(60000,)
```

train_images 的shape第一维度是60000，说明有6万个图片，然后标签第一维度也是6万与之对应。

```python
train_images = train_images.reshape((60000, 784))
train_images = train_images.astype('float32')
train_images = train_images / 255
```

第一步将第二维第三维数据合并到一维。

第二步是转换ndarray的dtype数据类型。

第三部是将数据0-255 归一化为 0- 1 。

类似的test_images也需要这样处理，这里就略过了。

标签数据需要进行one-hot编码处理：

```
from keras.utils import to_categorical
train_labels = to_categorical(train_labels)
train_labels[0]
array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], dtype=float32)
```

one-hot编码的具体解释这里略过了，其他地方会讨论的。



## 感知器

感知器就是一层或者说单层神经网络。感知器类似于逻辑回归模型，只能做线性分类任务。

单层神经网络的编写用Keras非常的简单，但如果用tensorflow还是需要写一些代码的。不过作为一开始推荐还是用tensorflow来写一个简单的单层神经网络。因为Keras是基于tensorflow的更高层模块，这对于我们理解Keras具体做了什么工作很有帮助，也能帮助我们理解具体单层神经网络进行了那些数学运算。

```python
x = tf.placeholder(tf.float32, [None, 784])
W = tf.Variable(tf.zeros([784, 10]))
b = tf.Variable(tf.zeros([10]))
y_true = tf.placeholder(tf.float32, [None, 10])
y_logits = tf.matmul(x, W) + b
y_pred = tf.nn.softmax(y_logits )
```

![img]({static}/images/机器学习/单层神经网络数学运算.png "单层神经网络数学运算")

输入参数x第二维度784对应权重矩阵第一维度784，通常神经网络权重矩阵W的shape是(前一层节点数, 后一层节点数) 。这样输入参数矩阵x和权重矩阵W进行矩阵乘法【张量的点积，np.dot运算】之后得到第二维度等于权重矩阵第二维度的矩阵。输出的值数据送入 y_logits。这里 `tf.matmul` 就是进行的矩阵的乘法运算。

这里 `tf.nn.softmax` 是激活函数，具体softmax激活函数的讨论这里略过了。



### 交叉熵

tensorflow提供了专门的交叉熵计算函数，这里我们先用更原始的计算公式来看一下（参考了 [这篇文章](http://wiki.jikexueyuan.com/project/tensorflow-zh/tutorials/mnist_beginners.html) ）：

```python
cross_entropy = -tf.reduce_sum(y_true * tf.log(y_pred))
```

大体过程如下所示：
$$
- \sum (1,0,0) * log((0.5,0.4,0.1)) = -(1*log0.5 + 0*log0.4 + 0*log0.1) = 0.301
$$
交叉熵越大那么预测值越偏离真实值，交叉熵越小那么预测值越接近真实值。

```python
train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy) # 0.01是学习速率
```

### 使用tensorflow自带的交叉熵方法

推荐使用tensorflow自带的softmax+交叉熵方法来计算交叉熵，参考了 [这篇文章](http://blog.csdn.net/behamcheung/article/details/71911133) ，说是计算会更稳定些。

现在让我们把到目前的代码整理一下：

```python
import tensorflow as tf
from keras.datasets import mnist
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()
train_images = train_images.reshape((60000, 784))
train_images = train_images.astype('float32')
train_images = train_images / 255
test_images = test_images.reshape((10000, 784))
test_images = test_images.astype('float32')
test_images = test_images / 255
from keras.utils import to_categorical
train_labels = to_categorical(train_labels)
test_labels = to_categorical(test_labels)
x = tf.placeholder(tf.float32, [None, 784])
W = tf.Variable(tf.zeros([784, 10]))
b = tf.Variable(tf.zeros([10]))
y_true = tf.placeholder(tf.float32, [None, 10])
y_logits = tf.matmul(x, W) + b
y_pred = tf.nn.softmax(y_logits )
cross_entropy = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(logits=y_logits , labels=y_true))
train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)
```

好了，我们的例子进入收尾阶段了：

```python
correct_mask = tf.equal(tf.argmax(y_pred, 1), tf.argmax(y_true, 1))
accuracy = tf.reduce_mean(tf.cast(correct_mask, tf.float32)) 
with tf.Session() as sess:

    # Train
    sess.run(tf.global_variables_initializer())

    count = 0
    for _ in range(128):
        batch_xs = train_images[count*128:128*(count+1)]
        batch_ys = train_labels[count*128:128*(count+1)]
        sess.run(train_step, feed_dict={x: batch_xs, y_true: batch_ys})
        count += 1

        if count % 10 == 0:
            ans = sess.run(accuracy, feed_dict={x: test_images, y_true: test_labels})
            print("Accuracy: {:.4}%".format(ans*100))
    # LAST
    ans = sess.run(accuracy, feed_dict={x: test_images, y_true: test_labels})
    print("Accuracy: {:.4}%".format(ans*100))
```

关于tf.argmax 函数请看下面的例子。不感兴趣的可以略过，其作用就是把标签解释出来，不是这里的重点。

```python
import tensorflow as tf
sess = tf.Session()
m = sess.run(tf.truncated_normal((5,10), stddev = 0.1) )

-----
array([[ 0.0919205 ,  0.06030607,  0.01196606,  0.03031359, -0.13546242,
        -0.12748787, -0.09680127,  0.12220833,  0.15264732,  0.05449662],
       [ 0.01277541, -0.00535311,  0.03589706,  0.01658093, -0.16726552,
        -0.06979545, -0.14876817, -0.03735523, -0.0439501 ,  0.15896702],
       [-0.05869294, -0.14986654, -0.17551927,  0.08360171, -0.00648978,
        -0.03274798, -0.05770732,  0.01505487,  0.13726853, -0.01670119],
       [-0.02666636, -0.05316785, -0.05433881, -0.02210794,  0.01175172,
        -0.0674843 , -0.06402522,  0.00812987, -0.17738222,  0.01375954],
       [-0.01734987,  0.01096244, -0.19889738,  0.08350741, -0.00222254,
         0.05094135,  0.06777989, -0.01986633, -0.1863249 , -0.04648132]],
      dtype=float32)
---

col_max = sess.run(tf.argmax(m, 0) )
----
array([0, 0, 1, 2, 3, 4, 4, 0, 0, 1], dtype=int64)
---
row_max = sess.run(tf.argmax(m, 1) ) 
---
array([8, 9, 8, 9, 3], dtype=int64)
---
```

所以 tf.argmax 第二个参数是1，那么返回一行数值最大的那个index索引值。 `tf.argmax(y_pred, 1)` 返回的那个索引值在本例中比较简单，就是实际预测的数字值。

tf.reduce_mean 将所有维度的元素相加然后求平均值

这个例子最后就是调用tensorflow的作业流程，启动运算数据流。然后评估一下对于测试数据现在精度如何了。这些不是这里的重点。就单层神经网络来说mnist例子很难超过90%的。



## 多层感知器

多层感知器实际上就是两层全连接神经网络。理论上两层神经网络可以无限逼近任意连续的函数了。下面用Keras来实现一个多层感知器。

首先我们试着把上面单层神经网络用Keras写一遍，下面是准备数据过程，后面都一样的。

### 准备数据

```python
import tensorflow as tf
from keras.datasets import mnist
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()
train_images = train_images.reshape((60000, 784))
train_images = train_images.astype('float32')
train_images = train_images / 255
test_images = test_images.reshape((10000, 784))
test_images = test_images.astype('float32')
test_images = test_images / 255
from keras.utils import to_categorical
train_labels = to_categorical(train_labels)
test_labels = to_categorical(test_labels)
```

### 建模

```python
from keras.models import Sequential
from keras.layers import Dense
model = Sequential()
model.add(Dense(10, activation='softmax', input_shape=(784,)))
model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])
model.fit(train_images, train_labels, epochs=5, batch_size=128)
```

```
Epoch 1/5
60000/60000 [==============================] - 1s 13us/step - loss: 0.6063 - acc: 0.8482
Epoch 2/5
60000/60000 [==============================] - 1s 12us/step - loss: 0.3316 - acc: 0.9083
Epoch 3/5
60000/60000 [==============================] - 1s 12us/step - loss: 0.3025 - acc: 0.9159
Epoch 4/5
60000/60000 [==============================] - 1s 11us/step - loss: 0.2889 - acc: 0.9194
Epoch 5/5
60000/60000 [==============================] - 1s 12us/step - loss: 0.2806 - acc: 0.9219
```



```
score = model.evaluate(test_images, test_labels, verbose=0)
print('Test loss:', score[0])
print('Test accuracy:', score[1])
```

```
Test loss: 0.2757013351589441
Test accuracy: 0.9232
```

结果大概也是差不多的。因为这个例子多运行几次epoch，但单层神经网络再怎么优化也只能到92%了。

上面的建模过程稍微加一行，我们就构建了一个多层感知器。一般多层神经网络前面的激活函数选relu会更好一些。也就多加了一层，最后输出节点数为10的神经网络。

```
from keras.models import Sequential
from keras.layers import Dense
model = Sequential()
model.add(Dense(512, activation='relu', input_shape=(784,)))
model.add(Dense(10, activation='softmax'))
model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])
model.fit(train_images, train_labels, epochs=5, batch_size=128)
```

```
Epoch 1/5
60000/60000 [==============================] - 5s 85us/step - loss: 0.2569 - acc: 0.9258
Epoch 2/5
60000/60000 [==============================] - 5s 84us/step - loss: 0.1037 - acc: 0.9688
Epoch 3/5
60000/60000 [==============================] - 5s 87us/step - loss: 0.0685 - acc: 0.9790
Epoch 4/5
60000/60000 [==============================] - 5s 85us/step - loss: 0.0508 - acc: 0.9848
Epoch 5/5
60000/60000 [==============================] - 5s 87us/step - loss: 0.0368 - acc: 0.9888
```

```
Test loss: 0.07560657636675751
Test accuracy: 0.9777
```

精度能够到97%了。

下面是Keras代码库examples里面的解决mnist问题的多层感知器，我根据上面的讨论将代码稍微调整下，建模过程如下：

```python
from keras.models import Sequential
from keras.layers import Dense,Dropout
from keras.optimizers import RMSprop
model = Sequential()
model.add(Dense(512, activation='relu', input_shape=(784,)))
model.add(Dense(512, activation='relu'))
model.add(Dense(10, activation='softmax'))
model.summary()
```

区别就是又加了一层神经网络。

```
Epoch 1/5
60000/60000 [==============================] - 8s 139us/step - loss: 0.2197 - acc: 0.9320
Epoch 2/5
60000/60000 [==============================] - 8s 140us/step - loss: 0.0815 - acc: 0.9750
Epoch 3/5
60000/60000 [==============================] - 9s 145us/step - loss: 0.0530 - acc: 0.9836
Epoch 4/5
60000/60000 [==============================] - 9s 151us/step - loss: 0.0374 - acc: 0.9886
Epoch 5/5
60000/60000 [==============================] - 9s 151us/step - loss: 0.0302 - acc: 0.9899
```

```
Test loss: 0.08149926771794035
Test accuracy: 0.9808
```

examples里面还新加入了Dropout层，这个是一种过拟合技术，我们加上之后会如何：

```python
from keras.models import Sequential
from keras.layers import Dense,Dropout
from keras.optimizers import RMSprop
model = Sequential()
model.add(Dense(512, activation='relu', input_shape=(784,)))
model.add(Dropout(0.2))
model.add(Dense(512, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(10, activation='softmax'))
model.summary()
```

```
Epoch 1/5
60000/60000 [==============================] - 10s 162us/step - loss: 0.2439 - acc: 0.9253
Epoch 2/5
60000/60000 [==============================] - 10s 169us/step - loss: 0.1031 - acc: 0.9688
Epoch 3/5
60000/60000 [==============================] - 9s 151us/step - loss: 0.0760 - acc: 0.9771
Epoch 4/5
60000/60000 [==============================] - 10s 165us/step - loss: 0.0604 - acc: 0.9817
Epoch 5/5
60000/60000 [==============================] - 9s 155us/step - loss: 0.0512 - acc: 0.9846
```

```
Test loss: 0.07319687194137806
Test accuracy: 0.9814
```

区别其实不大，至少就mnist来说提升最大的是又新加入了一层神经网络，加入Dropout层没看出区别。



## 多层神经网络或者深度学习

卷积神经网络相关后面的讨论补上，这里我们主要来看下Keras代码库examples里面介绍的用CNN，深度学习神经网络来解决mnist问题效果如何。具体理解后面再说。

```
import tensorflow as tf
from keras.datasets import mnist
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()
```

```
from keras import backend as K
if K.image_data_format() == 'channels_first':
    train_images = train_images.reshape(60000, 1, 28, 28)
    test_images = test_images.reshape(10000, 1, 28, 28)
    input_shape = (1, 28, 28)
else:
    train_images = train_images.reshape(60000, 28, 28, 1)
    test_images = test_images.reshape(10000, 28, 28, 1)
    input_shape = (28, 28, 1)
```

这里似乎涉及到不同backend的图形维度选择问题，这个后面再说。

```
train_images = train_images.astype('float32')
train_images = train_images / 255
test_images = test_images.astype('float32')
test_images = test_images / 255
from keras.utils import to_categorical
train_labels = to_categorical(train_labels)
test_labels = to_categorical(test_labels)
```

继续之前的数据预处理。

```python
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten
from keras.layers import Conv2D, MaxPooling2D
from keras.optimizers import Adadelta
model = Sequential()
model.add(Conv2D(32, activation='relu', input_shape=input_shape, kernel_size=(3,3)))
model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(10, activation='softmax'))
model.summary()
```

本例子跑起来开始有点慢了。Dropout应该不算一层，Flatten我估计不算一层，那么上面例子大概有5层。

```
Epoch 1/5
60000/60000 [==============================] - ETA: 0s - loss: 0.2683 - acc: 0.917 - 119s 2ms/step - loss: 0.2683 - acc: 0.9173
Epoch 2/5
60000/60000 [==============================] - 120s 2ms/step - loss: 0.0891 - acc: 0.9734
Epoch 3/5
60000/60000 [==============================] - 115s 2ms/step - loss: 0.0655 - acc: 0.9804
Epoch 4/5
60000/60000 [==============================] - 111s 2ms/step - loss: 0.0555 - acc: 0.9833
Epoch 5/5
60000/60000 [==============================] - 114s 2ms/step - loss: 0.0466 - acc: 0.9856
```

```
Test loss: 0.031076731445921907
Test accuracy: 0.9898
```

例子报道说epochs=12的时候精度能够上升到99%。

为了公平起见，绝对多层感知器和CNN神经网络这两个例子都按照epochs=12再跑一次来对比一下看看。

多层感知器：

```
Test loss: 0.08958661408673184
Test accuracy: 0.9821
```

和跑5次没有区别了。

CNN的看了一下个人PC CPU基本上跑满了，然后GPU没怎么用，tensorflow决定换成tensorflow-gpu 【PS：注意之前你用pip安装tensorflow了的，再安装个tensorflow-gpu即可，原来那个tensorflow包不能删的。】然后再看下。然后发现我这里显卡写着Intel UHD，似乎只有NAVID才能开启gpu，算了。

CNN神经网络：

```
Test loss: 0.028602463609369078
Test accuracy: 0.992
```

精度提升到了99%，看来CNN多训练几次后续效果还能提升，别小看了这1%的提升啊！



### 保存模型

一次训练有点费时了，那么如何保存训练好的模型呢？这个问题在keras文档FAQ里面有，算是很经典的一个问题了。

```
model.save('my_model.h5')
```

保存的数据有：

- 模型的结构，方便重新创造模型
- 模型训练得到的权重数据
- 训练损失和优化器配置
- 优化器状态，允许继续上一次训练

下次使用模型如下所示：

```python
import tensorflow as tf
from keras.datasets import mnist
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()

from keras import backend as K
if K.image_data_format() == 'channels_first':
    test_images = test_images.reshape(10000, 1, 28, 28)
    input_shape = (1, 28, 28)
else:
    test_images = test_images.reshape(10000, 28, 28, 1)
    input_shape = (28, 28, 1)
    

test_images = test_images.astype('float32')
test_images = test_images / 255
from keras.utils import to_categorical
test_labels = to_categorical(test_labels)
```

```
from keras.models import load_model
model = load_model('cnn_for_mnist_model.h5')
```

```
score = model.evaluate(test_images, test_labels, verbose=0)
print('Test loss:', score[0])
print('Test accuracy:', score[1])
```

```
Test loss: 0.028602463609369078
Test accuracy: 0.992
```





## 参考资料

1. 机器学习实战 Peter Harrington 著 李锐 李鹏等译
2. [机器学习实战线上教程](http://ml.apachecn.org/mlia/)
3. python深度学习 弗朗索瓦·肖奈
4. [deep learning 中文版](https://github.com/exacity/deeplearningbook-chinese)
5. 机器学习 周志华著
6. [这个文章介绍神经网络写的很好](https://www.cnblogs.com/subconscious/p/5058741.html)