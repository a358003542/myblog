Title: 机器学习第四谈
Slug: machine-learning-talk-four
Date: 2018-11-07
Modified: 2018-11-07
Tags: machine-learning

[TOC]

## 前言

在前面第三谈中谈论的典型的二分类问题，多分类问题和回归问题在机器学习中都属于监督学习。此外还有无监督学习，自监督学习和强化学习。

### 无监督学习

无监督学习是数据分析的必备技能，在解决监督学习问题之前，为了更好地了解数据集，它通常是一个必要步骤。

降维 聚类 是常见的无监督学习方法。

### 自监督学习

自编码器 (autoencoder) 是有名的自监督学习例子。



### 强化学习

智能体接受环境的信息，为了某种奖励最大化而学习选择行为。



## 机器学习通用工作流程

### 定义问题，收集数据集

- 你的输入数据是什么？你要预测什么？
- 你面对的是什么类型的问题？是二分类问题还是多分类问题等等。

### 选择衡量成功的指标

模型要优化什么，它应该直接和你的业务目标相关。

### 确定评估方法

- 留出验证集 数据量很大的时候采用 
  具体操作就是训练数据里面一部分作为训练集，剩下来的一部分作为验证集，然后用训练集训练，验证集评估当前模型的好坏。模型参数调节好训练好之后，记得最后用整个训练集从头训练一次，最后用另外的测试集数据测试下模型的实际效果。
- K折交叉验证
- 重复的K折验证

### 准备数据

#### 数据张量化

神经网络的所有输入和输出目标都必须是浮点数张量（某些情况下可以是整数张量）。无论面对的是声音，文本，图像还是视频，都必须先将其转换成为张量。

#### 数据标准化

之前我们看到的数据标准化情况有：

- 图像0-255数据整除缩减到 0-1 数据区间

- 如果是多个特征的数据，那么推荐如前面提及的进行z-score的标准化处理
```python
mean = train_data.mean(axis=0)
train_data -= mean
std = train_data.std(axis=0)
train_data /= std
```

#### 处理缺失值

对于神经网络来说，将缺失值设置为 `0` 是安全的。

#### 特征工程

比如自然语言处理，根据自然语言处理学到的知识，选择二元模型对输入数据进行再处理。

- 良好的特征工程仍然可以让你用更少的资源更优雅地解决问题
- 良好的特征可以让你用更少的数据解决问题



### 开发比基准更好的模型

简单来说就是先随便开发一个小模型，要求不要太高，但至少要比随机乱猜准确率要高点的模型。

### 扩大模型规模，开发过拟合模型

一般是：

- 添加更多的层
- 让每一层变得更大
- 训练更多次数

要始终监控训练损失和验证损失，如果你发现模型随着训练次数增加在验证数据上性能下降了，那么就出现了过拟合。

### 模型正则化与调节超参数

这一步是最费时间的：你将不断调节模型，训练，验证，再调节模型... 。下面是一些你可能尝试的手段

#### 添加Dropout层

Dropout层是深度学习之父Hinton和他的学生首次提出来的，它的原理很简单：对某一层使用dropout即该层在训练的时候会随机舍弃一些输出特征（也就是值变为0）。dropout比率是被设为0的特征所占的比例，一般设0.2~0.5之间。

添加Dropout层是最有效也最常用的正则化方法——正则化指降低过拟合。

#### 尝试增加或者减少层

一般实践中开始会选择较少的层和节点数，然后逐渐增加之，直到这种增加对验证损失影响变得很小。

#### 尝试L1 L2正则化

L1正则化和L2正则化都属于权重正则化，这是一种降低过拟合的方法，强制让模型权重只能取较小的值。



#### 尝试不同的超参数

如每层的单元个数，优化器的学习率等。



## 参考资料

1. 机器学习实战 Peter Harrington 著 李锐 李鹏等译
2. [机器学习实战线上教程](http://ml.apachecn.org/mlia/)
3. python深度学习 弗朗索瓦·肖奈