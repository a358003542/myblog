Category: algorithm
Tags: algorithm, 


[TOC]

## 前言

蒙特卡罗模拟的蒙特卡罗是一个赌运气游戏的名字，而蒙特卡罗模拟在现代计算机发展到今天的背景下，这种思想已经越来越重要了。简单来说其试图将一个问题转成某种随机过程，然后用计算机来模拟这个随机过程从而得到解答。因为自然界存在着大量的问题，其本身就暗含随机性的，对于这样的过程对应建立基于随机性的模型是很直观的，但还存在着某些问题，比如计算 $\pi$ 值，和随机性不是很相关的，但通过某个随机过程，也能计算出 $\pi$ 值，这就是典型的蒙特卡罗模拟，或者叫做蒙特卡罗方法。

比如我们常见的随机游走问题，直接对其建模就是一个蒙特卡罗模拟过程。

本文除了讨论随机游走和一些随机过程，还会涉及到线性拟合，最小二乘法，线性回归和相关绘图等内容。

基本的建模代码如下所示，参考了MIT的python编程导论一书：

```python

import random
from math import sqrt


class Location(object):
    def __init__(self, x, y):
        self.x = x
        self.y = y

    def move(self, dx, dy):
        return Location(self.x + dx, self.y + dy)

    def get_x(self):
        return self.x

    def get_y(self):
        return self.y

    def distance(self, other):
        ox, oy = other.x, other.y
        distance = sqrt((self.x - ox) ** 2 + (self.y - oy) ** 2)
        return distance

    def __str__(self):
        return f'<Location ({self.x}, {self.y})>'


class Drunk(object):
    def __init__(self, name=None):
        self.name = name

    def __str__(self):
        if self.name is not None:
            return self.name
        else:
            return 'Anonymous'


class UsualDrunk(Drunk):
    def take_step(self):
        step_choices = [(0, 1), (0, -1), (1, 0), (-1, 0)]
        return random.choice(step_choices)


class Field(object):
    def __init__(self):
        self.drunks = {}

    def add_drunk(self, drunk, loc):
        if isinstance(loc, (tuple, list)):
            assert len(loc) == 2
            loc = Location(loc[0], loc[1])

        if drunk in self.drunks:
            raise ValueError('Duplicate drunk')
        else:
            self.drunks[drunk] = loc

    def move_drunk(self, drunk):
        if drunk not in self.drunks:
            raise ValueError('Drunk not in field')

        dx, dy = drunk.take_step()
        current_loc = self.drunks[drunk]
        self.drunks[drunk] = current_loc.move(dx, dy)

    def get_loc(self, drunk):
        if drunk not in self.drunks:
            raise ValueError('Drunk not in field')

        return self.drunks[drunk]


def walk(f, d, num_steps):
    start = f.get_loc(d)
    for s in range(num_steps):
        f.move_drunk(d)

    return start.distance(f.get_loc(d))


def bulk_walk(num_steps, num_bulk, dClass):
    """
    :param num_steps: 随机行走了多少步
    :param num_bulk: 一批次进行了多少次实验
    :param dClass: 醉汉类型
    :return: distances 一批次里面每次开图的总共行走距离列表
    """
    drunk = dClass()
    origin = Location(0, 0)
    distances = []
    for i in range(num_bulk):
        f = Field()
        f.add_drunk(drunk, origin)
        distances.append(round(walk(f, drunk, num_steps), 1))
    return distances


def drunk_test(num_steps_batch, num_bulk, dClass):
    """

    :param num_steps_batch: 随机行走多少步填入批次
    :param num_bulk: 一批次进行了多少次实验
    :param dClass: 醉汉类型
    :return:
    """
    mean_distance_list = []
    for num_steps in num_steps_batch:
        distances = bulk_walk(num_steps, num_bulk, dClass)
        print(f'{dClass.__name__} random walk of {num_steps} steps')
        mean_distance = round(sum(distances) / len(distances), 4)
        mean_distance_list.append(mean_distance)
        print(f'Mean = {mean_distance}')
        print(f'Max = {max(distances)} Min = {min(distances)}')

    return mean_distance_list


if __name__ == '__main__':
    num_steps_batch = list(range(100, 3000, 100))
    data = drunk_test(num_steps_batch, 100, UsualDrunk)

```

就作为代码都是很简单直观的一些代码。然后我们定义了如下 绘图函数：

```python

def polyfit_plot(ax, x, y, deg=1, xlabel='', ylabel='', title='', **kwargs):
    """
    多项式拟合绘图
    :return:
    """
    if xlabel:
        ax.set_xlabel(xlabel)
    if ylabel:
        ax.set_ylabel(ylabel)
    if title:
        ax.set_title(title)

    predict_func = np.poly1d(np.polyfit(x, y, deg))

    out = ax.plot(x, y, '.', x, predict_func(x), '-', **kwargs)
    return out


```

![1554721266217]({static}/images/arithmetic/random_walk_1.png)



上面绘制这个简单的线性拟合线的时候，我想到了很多东西，现在热门的机器学习，和随机过程模拟，统计过程分析，基本作图演示等概念都是密不可分的。

比如说上面简单的一元函数拟合，在numpy提供的polyfit函数里面，本身就支持多次的，也就是直接就可以做多项式曲线拟合的。然后这个拟合过程所使用的方法叫做 **最小二乘法** ，其内在就是构建了一个 函数：让这个函数值最小：
$$
\sum (y - y_{predict})^2
$$
这个函数在这里还只是叫做误差函数。在机器学习那边就叫做平方损失函数，然后在机器学习里面我们会接触到更多的损失函数，而梯度下降算法扮演的角色等同于最小二乘法，让目标损失函数的值最小，然后获得一些参数。这些参数扔进我们的多项式里面，就成了我们所谓的模型。

实际上所谓的线性回归 多项式回归具体过程也是和上面差不多的，只是还提供了predict等操作，然后思路要换成机器学习的那种建模说法等。

机器学习中的线性回归会引入更多的数据和更多的特征变量，建模会更加复杂等等，但大体过程也就是类似上面谈及的。

当然这里说一句题外话，虽然很多人对现在的深度学习都抱有这种觉得不过是一种统计学的轻蔑态度，我觉得就过了，正所谓量变到质变，有些东西做的更加复杂之后就和原本那简单的回归是大相径庭的，这里不单单指更加厉害的任意曲线的拟合能力，而是更多的质变在这里面，当然从学习角度这里慢慢去理解是没问题的。

就随机游走问题个人觉得没有继续深究了，下面除了解决计算 $\pi$ 的问题之外，再下面对更多的概率统计知识深入理解之外，再介绍更多的函数拟合和绘图，就差不多该顺理成章转到机器学习那块去了，去建立更多的机器学习模型等，来解决更多实际问题。或者说的更牛掰一些的，模拟和计算世界。





